{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7361176,"sourceType":"datasetVersion","datasetId":4275849},{"sourceId":7377670,"sourceType":"datasetVersion","datasetId":4287259},{"sourceId":7677600,"sourceType":"datasetVersion","datasetId":4478819}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"d7a232f6-4a2d-49ef-96f5-a15dd3bcaeea","_cell_guid":"149d6e45-65ce-4ec0-91f0-8ba62cf8ae73","collapsed":false,"scrolled":true,"execution":{"iopub.status.busy":"2024-04-02T12:13:08.880374Z","iopub.execute_input":"2024-04-02T12:13:08.881023Z","iopub.status.idle":"2024-04-02T12:13:08.886237Z","shell.execute_reply.started":"2024-04-02T12:13:08.880989Z","shell.execute_reply":"2024-04-02T12:13:08.885212Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = 2\ninput_shape = (72, 72, 3)\ndataset_path = \"/kaggle/input/labeledhpcd/hpcd/training\"\n\ndef load_images_from_folder(folder):\n    images = []\n    labels = []\n    for label, class_folder in enumerate(os.listdir(folder)):\n        class_path = os.path.join(folder, class_folder)\n        for filename in os.listdir(class_path):\n            img = cv2.imread(os.path.join(class_path, filename))\n            if img is not None:\n                img = cv2.resize(img, (72, 72))  \n                images.append(img)\n                labels.append(label)\n    return images, labels\n\nx_train, y_train = load_images_from_folder(dataset_path)\nx_train = np.array(x_train)\ny_train = np.array(y_train)\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)","metadata":{"_uuid":"2f923563-2821-4b7b-9250-6144290d2cf3","_cell_guid":"c9331ea8-7b44-4ccd-a7aa-4cafe9625b43","collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T12:18:34.831298Z","iopub.execute_input":"2024-04-02T12:18:34.831715Z","iopub.status.idle":"2024-04-02T12:28:29.961086Z","shell.execute_reply.started":"2024-04-02T12:18:34.831685Z","shell.execute_reply":"2024-04-02T12:28:29.960231Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.001\nweight_decay = 0.0001\nbatch_size = 256\nnum_epochs = 100\nimage_size = 72  \npatch_size = 6  \nnum_patches = (image_size // patch_size) ** 2\nprojection_dim = 64\nnum_heads = 4\ntransformer_units = [\n    projection_dim * 2,\n    projection_dim,\n]  \ntransformer_layers = 8\nmlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier","metadata":{"_uuid":"60e38987-6f5d-4f5a-9971-a75820bb8c83","_cell_guid":"38f0c52c-e39d-4c62-a371-d0472d28e663","collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T12:30:58.261447Z","iopub.execute_input":"2024-04-02T12:30:58.261853Z","iopub.status.idle":"2024-04-02T12:30:58.267953Z","shell.execute_reply.started":"2024-04-02T12:30:58.261823Z","shell.execute_reply":"2024-04-02T12:30:58.266977Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_augmentation = keras.Sequential(\n    [\n        layers.Normalization(),\n        layers.Resizing(image_size, image_size),\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomRotation(factor=0.02),\n        layers.RandomZoom(\n            height_factor=0.2, width_factor=0.2\n        ),\n    ],\n    name=\"data_augmentation\",\n)\ndata_augmentation.layers[0].adapt(x_train)","metadata":{"_uuid":"6d044345-51ff-4476-b5a9-a584937e41c0","_cell_guid":"7ac18264-0c65-4c5b-8882-b3e41d277973","collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T12:31:00.667280Z","iopub.execute_input":"2024-04-02T12:31:00.668013Z","iopub.status.idle":"2024-04-02T12:31:10.162477Z","shell.execute_reply.started":"2024-04-02T12:31:00.667978Z","shell.execute_reply":"2024-04-02T12:31:10.161611Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mlp(x, hidden_units, dropout_rate):\n    for units in hidden_units:\n        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n        x = layers.Dropout(dropout_rate)(x)\n    return x","metadata":{"_uuid":"bbe11f56-9e65-460b-98c5-23b75607d6ab","_cell_guid":"f520afe2-4229-4dc0-9f69-2ca1301de85a","collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T12:31:43.469398Z","iopub.execute_input":"2024-04-02T12:31:43.470150Z","iopub.status.idle":"2024-04-02T12:31:43.475296Z","shell.execute_reply.started":"2024-04-02T12:31:43.470116Z","shell.execute_reply":"2024-04-02T12:31:43.474260Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Patches(layers.Layer):\n    def __init__(self, patch_size):\n        super().__init__()\n        self.patch_size = patch_size\n\n    def call(self, images):\n        batch_size = tf.shape(images)[0]\n        patches = tf.image.extract_patches(\n            images=images,\n            sizes=[1, self.patch_size, self.patch_size, 1],\n            strides=[1, self.patch_size, self.patch_size, 1],\n            rates=[1, 1, 1, 1],\n            padding=\"VALID\",\n        )\n        patch_dims = patches.shape[-1]\n        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n        return patches","metadata":{"_uuid":"50c5e37f-ba49-470c-afa0-8db833de96f9","_cell_guid":"16126d1c-d5c6-46fb-93d1-a39182fd967a","collapsed":false,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-02T12:31:44.510928Z","iopub.execute_input":"2024-04-02T12:31:44.511930Z","iopub.status.idle":"2024-04-02T12:31:44.518512Z","shell.execute_reply.started":"2024-04-02T12:31:44.511892Z","shell.execute_reply":"2024-04-02T12:31:44.517635Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(4, 4))\nimage = x_train[np.random.choice(range(x_train.shape[0]))]\nplt.imshow(image.astype(\"uint8\"))\nplt.axis(\"off\")\n\nresized_image = tf.image.resize(\n    tf.convert_to_tensor([image]), size=(image_size, image_size)\n)\npatches = Patches(patch_size)(resized_image)\nprint(f\"Image size: {image_size} X {image_size}\")\nprint(f\"Patch size: {patch_size} X {patch_size}\")\nprint(f\"Patches per image: {patches.shape[1]}\")\nprint(f\"Elements per patch: {patches.shape[-1]}\")\n\nn = int(np.sqrt(patches.shape[1]))\nplt.figure(figsize=(4, 4))\nfor i, patch in enumerate(patches[0]):\n    ax = plt.subplot(n, n, i + 1)\n    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n    plt.axis(\"off\")","metadata":{"_uuid":"60bce92a-0a8c-440b-9b53-a06c4aa17d57","_cell_guid":"afc52d52-80d3-44f1-9a16-83d8fb8ec8dc","collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T12:31:53.221416Z","iopub.execute_input":"2024-04-02T12:31:53.221770Z","iopub.status.idle":"2024-04-02T12:31:57.745940Z","shell.execute_reply.started":"2024-04-02T12:31:53.221743Z","shell.execute_reply":"2024-04-02T12:31:57.744914Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PatchEncoder(layers.Layer):\n    def __init__(self, num_patches, projection_dim):\n        super().__init__()\n        self.num_patches = num_patches\n        self.projection = layers.Dense(units=projection_dim)\n        self.position_embedding = layers.Embedding(\n            input_dim=num_patches, output_dim=projection_dim\n        )\n\n    def call(self, patch):\n        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n        encoded = self.projection(patch) + self.position_embedding(positions)\n        return encoded","metadata":{"_uuid":"b195eff4-a472-4dcb-985f-3528e8492bcd","_cell_guid":"2e407f99-2834-410c-a763-91949dff082b","collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T12:31:58.648346Z","iopub.execute_input":"2024-04-02T12:31:58.649309Z","iopub.status.idle":"2024-04-02T12:31:58.656282Z","shell.execute_reply.started":"2024-04-02T12:31:58.649266Z","shell.execute_reply":"2024-04-02T12:31:58.655363Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def create_vit_classifier():\n#     inputs = layers.Input(shape=input_shape)\n#     # Augment data.\n#     augmented = data_augmentation(inputs)\n#     # Create patches.\n#     patches = Patches(patch_size)(augmented)\n#     # Encode patches.\n#     encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n\n#     # Create multiple layers of the Transformer block.\n#     for _ in range(transformer_layers):\n#         # Layer normalization 1.\n#         x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n#         # Create a multi-head attention layer.\n#         attention_output = layers.MultiHeadAttention(\n#             num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n#         )(x1, x1)\n#         # Skip connection 1.\n#         x2 = layers.Add()([attention_output, encoded_patches])\n#         # Layer normalization 2.\n#         x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n#         # MLP.\n#         x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n#         # Skip connection 2.\n#         encoded_patches = layers.Add()([x3, x2])\n\n#     # Create a [batch_size, projection_dim] tensor.\n#     representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n#     representation = layers.Flatten()(representation)\n#     representation = layers.Dropout(0.5)(representation)\n#     # Add MLP.\n#     features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n#     # Classify outputs.\n#     logits = layers.Dense(num_classes)(features)\n#     # Create the Keras model.\n#     model = keras.Model(inputs=inputs, outputs=logits)\n#     return model\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ndef create_vit_classifier():\n    inputs = layers.Input(shape=input_shape)\n    augmented = data_augmentation(inputs)\n    patches = Patches(patch_size)(augmented)\n    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n\n    \n    for _ in range(transformer_layers):\n        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n        \n        attention_output = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n        )(x1, x1)\n        \n        x2 = layers.Add()([attention_output, encoded_patches])\n        \n        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n        \n        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n        \n        encoded_patches = layers.Add()([x3, x2])\n\n    \n    extra_attention_output = layers.MultiHeadAttention(\n        num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n    )(encoded_patches, encoded_patches)\n    encoded_patches = layers.Add()([extra_attention_output, encoded_patches])\n\n    \n    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n    representation = layers.Flatten()(representation)\n    representation = layers.Dropout(0.5)(representation)\n    \n    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n    \n    logits = layers.Dense(num_classes)(features)\n    \n    model = keras.Model(inputs=inputs, outputs=logits)\n    return model","metadata":{"_uuid":"7c089290-7be1-4871-9d38-526b7fc366c8","_cell_guid":"872c0626-4ce0-4ee1-9d6a-06eb3a0cd9ea","collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T12:32:54.648359Z","iopub.execute_input":"2024-04-02T12:32:54.649104Z","iopub.status.idle":"2024-04-02T12:32:54.661569Z","shell.execute_reply.started":"2024-04-02T12:32:54.649063Z","shell.execute_reply":"2024-04-02T12:32:54.660283Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vit = create_vit_classifier()\nvit.summary()","metadata":{"_uuid":"595c1263-995a-4b48-82ab-210851e9ae5c","_cell_guid":"72796b06-c2cc-4222-bc25-20a1c3f21685","collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T12:33:11.805782Z","iopub.execute_input":"2024-04-02T12:33:11.806750Z","iopub.status.idle":"2024-04-02T12:33:13.646464Z","shell.execute_reply.started":"2024-04-02T12:33:11.806711Z","shell.execute_reply":"2024-04-02T12:33:13.641655Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import tensorflow as tf\n# from tensorflow.keras import layers\n# from tensorflow.keras import models\n# from tensorflow.keras import optimizers\n# from tensorflow.keras import callbacks\n# import tensorflow_addons as tfa\n\n\n# def run_experiment(model):\n#     optimizer = tfa.optimizers.AdamW(\n#         learning_rate=learning_rate, weight_decay=weight_decay\n#     )\n\n#     model.compile(\n#         optimizer=optimizer,\n#         loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n#         metrics=[\n#             tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n#             tf.keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n#         ],\n#     )\n\n#     checkpoint_filepath = \"ViT_Save\"\n#     checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n#         checkpoint_filepath,\n#         monitor=\"val_accuracy\",\n#         save_best_only=True,\n#         save_weights_only=True,\n#     )\n\n#     history = model.fit(\n#         x=x_train,\n#         y=y_train,\n#         batch_size=batch_size,\n#         epochs=num_epochs,\n#         validation_split=0.1,\n#         callbacks=[checkpoint_callback]  # Add the checkpoint callback to save weights\n#     )\n\n#     # Save the entire model (architecture + weights)\n#     model.save(\"ViT_Save/trained_model\")\n\n#     _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n#     print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n#     print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n\n#     return history\n\n# vit_classifier = create_vit_classifier()\n# history = run_experiment(vit_classifier)\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import callbacks\nimport tensorflow_addons as tfa\nimport pickle\n\ndef run_experiment(model):\n    optimizer = tfa.optimizers.AdamW(\n        learning_rate=learning_rate, weight_decay=weight_decay\n    )\n\n    model.compile(\n        optimizer=optimizer,\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        metrics=[\n            tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n            tf.keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n        ],\n    )\n\n    checkpoint_filepath = \"ViT_Save\"\n    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n        checkpoint_filepath,\n        monitor=\"val_accuracy\",\n        save_best_only=True,\n        save_weights_only=True,\n    )\n\n    history = model.fit(\n        x=x_train,\n        y=y_train,\n        batch_size=batch_size,\n        epochs=num_epochs,\n        validation_split=0.1,\n        callbacks=[checkpoint_callback]  # Add the checkpoint callback to save weights\n    )\n\n    # Save the entire model (architecture + weights) as a pickle file\n    with open(\"model.pickle\", \"wb\") as f:\n        pickle.dump(model, f)\n\n    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n\n    return history\n\nvit_classifier = create_vit_classifier()\nhistory = run_experiment(vit_classifier)","metadata":{"_uuid":"43a54d89-4e42-4c49-99b9-0583273b4bc3","_cell_guid":"3d7001fa-c3f4-4227-abc8-45772c88c9bd","collapsed":false,"scrolled":true,"execution":{"iopub.status.busy":"2024-04-02T12:33:14.354620Z","iopub.execute_input":"2024-04-02T12:33:14.355286Z","iopub.status.idle":"2024-04-02T15:23:35.388815Z","shell.execute_reply.started":"2024-04-02T12:33:14.355240Z","shell.execute_reply":"2024-04-02T15:23:35.387970Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vit.save('vit_model.h5')","metadata":{"_uuid":"06d3caf1-de2f-469a-96e4-48d24c3a423d","_cell_guid":"d19d8eac-eb2f-4069-ba9c-d59c2c62a9b6","collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T16:01:18.689716Z","iopub.execute_input":"2024-04-02T16:01:18.690143Z","iopub.status.idle":"2024-04-02T16:01:18.957641Z","shell.execute_reply.started":"2024-04-02T16:01:18.690103Z","shell.execute_reply":"2024-04-02T16:01:18.956592Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_model = keras.saving.load_model(\"/kaggle/input/h5file/vit_model.h5\")","metadata":{"_uuid":"323681de-9a67-4e32-a1ba-1d865aeb3d92","_cell_guid":"ef7c2897-236e-408f-a557-8a89fec342ad","collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T16:01:27.511171Z","iopub.execute_input":"2024-04-02T16:01:27.511534Z","iopub.status.idle":"2024-04-02T16:01:28.527673Z","shell.execute_reply.started":"2024-04-02T16:01:27.511506Z","shell.execute_reply":"2024-04-02T16:01:28.526181Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def plot_training_history(history):\n#     # Plot training & validation accuracy values\n#     plt.plot(history.history['accuracy'])\n#     plt.plot(history.history['val_accuracy'])\n#     plt.title('Model accuracy')\n#     plt.xlabel('Epoch')\n#     plt.ylabel('Accuracy')\n#     plt.legend(['Train', 'Validation'], loc='upper left')\n#     plt.show()\n\n#     # Plot training & validation loss values\n#     plt.plot(history.history['loss'])\n#     plt.plot(history.history['val_loss'])\n#     plt.title('Model loss')\n#     plt.xlabel('Epoch')\n#     plt.ylabel('Loss')\n#     plt.legend(['Train', 'Validation'], loc='upper left')\n#     plt.show()\n    \n# plot_training_history(history)    \n\n\n\nimport matplotlib.pyplot as plt\n\ndef plot_training_history(history, save_path=None):\n    # Plot training & validation accuracy values\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('Model accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.show()\n\n    # Save the accuracy plot\n    if save_path:\n        accuracy_plot_path = save_path + '/accuracy_plot.png'\n        plt.savefig(accuracy_plot_path)\n        print(f\"Accuracy plot saved at: {accuracy_plot_path}\")\n\n    # Plot training & validation loss values\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.show()\n\n    # Save the loss plot\n    if save_path:\n        loss_plot_path = save_path + '/loss_plot.png'\n        plt.savefig(loss_plot_path)\n        print(f\"Loss plot saved at: {loss_plot_path}\")\n\n# Assuming 'history' is your training history\n# You can provide the Kaggle directory path where you want to save the plots\nkaggle_directory_path = '/kaggle/working'\nplot_training_history(history, save_path=kaggle_directory_path)","metadata":{"_uuid":"d6627143-df7b-4b6b-ade4-f6ab0f0119f8","_cell_guid":"a922e816-cec7-4819-b732-cdc01b1b5f40","collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T16:01:52.978817Z","iopub.execute_input":"2024-04-02T16:01:52.979655Z","iopub.status.idle":"2024-04-02T16:01:53.446636Z","shell.execute_reply.started":"2024-04-02T16:01:52.979614Z","shell.execute_reply":"2024-04-02T16:01:53.445722Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\n\nnew_model = load_model('/kaggle/input/resultshpcd/ViT_Save/trained_model')","metadata":{"_uuid":"06d35d7b-0337-45de-b5a0-f0c674cbcd50","_cell_guid":"d979975c-97e0-4918-a819-8eb6c3b43aa9","collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T16:02:47.681341Z","iopub.execute_input":"2024-04-02T16:02:47.682055Z","iopub.status.idle":"2024-04-02T16:02:55.277341Z","shell.execute_reply.started":"2024-04-02T16:02:47.682019Z","shell.execute_reply":"2024-04-02T16:02:55.276549Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model.evaluate(x_test,y_test)","metadata":{"_uuid":"92ebb420-46f7-4d50-9769-68c5212b7952","_cell_guid":"211ecfa6-717b-43d6-a17e-409666721e8b","collapsed":false,"scrolled":true,"execution":{"iopub.status.busy":"2024-04-02T16:03:00.393027Z","iopub.execute_input":"2024-04-02T16:03:00.393865Z","iopub.status.idle":"2024-04-02T16:03:20.175118Z","shell.execute_reply.started":"2024-04-02T16:03:00.393830Z","shell.execute_reply":"2024-04-02T16:03:20.174278Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.predict(x_test)","metadata":{"_uuid":"67569f21-58be-436c-84d7-3a60f94ca9df","_cell_guid":"e7473c93-2cee-4496-a673-1c9397b427ad","collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom keras.models import load_model\nimport matplotlib.pyplot as plt\n\n# model = load_model('ViT_Save/trained_model')\n\n\nindex = 2  \nsingle_test_data = x_test[index]\ntrue_label = y_test[index]\n\n\nprocessed_test_data = cv2.resize(single_test_data, (72, 72))  # Resize to match input_shape\nprocessed_test_data = np.expand_dims(processed_test_data, axis=0)  # Add batch dimension\n\n\npredictions = new_model.predict(processed_test_data)\n\n\npredicted_class = np.argmax(predictions)\n\nplt.imshow(single_test_data)\nplt.title(f'True label: {true_label}, Predicted label: {predicted_class}')\nplt.show()","metadata":{"_uuid":"9d2e8d1c-70b6-497f-85d8-35e786444a79","_cell_guid":"ae9a0b46-9d79-4706-8974-a6df63676fb8","collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T16:03:38.730047Z","iopub.execute_input":"2024-04-02T16:03:38.730914Z","iopub.status.idle":"2024-04-02T16:03:40.322301Z","shell.execute_reply.started":"2024-04-02T16:03:38.730881Z","shell.execute_reply":"2024-04-02T16:03:40.321393Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom keras.models import load_model\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n\nall_test_data = x_test\nall_true_labels = y_test\n\n\n# model = load_model('ViT_Save/trained_model')\n\n\nall_predicted_labels = []\n\n\nprocessed_test_data = np.array([cv2.resize(img, (72, 72)) for img in all_test_data])\n\n\npredictions = new_model.predict(processed_test_data)\n\n\nall_predicted_labels = np.argmax(predictions, axis=1)\n\n\nall_true_labels = np.array(all_true_labels)\n\n\nconf_matrix = confusion_matrix(all_true_labels, all_predicted_labels)\n\n\ndisp = ConfusionMatrixDisplay(conf_matrix, display_labels=np.unique(all_true_labels))\ndisp.plot(cmap=plt.cm.Blues, values_format='d')\nplt.show()","metadata":{"_uuid":"050f1a99-70e6-4e73-9e2c-764b52bf11b2","_cell_guid":"1ab6d5c8-aa62-41d4-bfc4-4a168945242b","collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T16:03:58.716493Z","iopub.execute_input":"2024-04-02T16:03:58.717231Z","iopub.status.idle":"2024-04-02T16:04:17.126261Z","shell.execute_reply.started":"2024-04-02T16:03:58.717200Z","shell.execute_reply":"2024-04-02T16:04:17.125158Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\nall_true_labels = np.array(all_true_labels)\npredictions_prob = predictions[:, 1]  \n\n\nfpr, tpr, _ = roc_curve(all_true_labels, predictions_prob)\nroc_auc = auc(fpr, tpr)\n\n# Plot ROC curve\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"_uuid":"da8be857-5f95-429f-bfcd-660703e9611e","_cell_guid":"6de9bc1b-6f9d-414e-8117-f9cb0016d8d7","collapsed":false,"execution":{"iopub.status.busy":"2024-04-02T17:05:58.058357Z","iopub.execute_input":"2024-04-02T17:05:58.059178Z","iopub.status.idle":"2024-04-02T17:05:58.278946Z","shell.execute_reply.started":"2024-04-02T17:05:58.059143Z","shell.execute_reply":"2024-04-02T17:05:58.278073Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}